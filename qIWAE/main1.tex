\documentclass{article} % For LaTeX2e
\usepackage{iclr2017_workshop,times}
\usepackage{hyperref}
\usepackage{url}

% my imports
\usepackage{amssymb,amsmath,amsthm}
\usepackage[makeroom]{cancel}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{multirow,bigstrut}
\newcommand{\eqname}[1]{\tag*{#1}}

\title{Reinterpreting and Extending \\ Importance-Weighted Autoencoders}


\author{Chris Cremer, Quaid Morris \& David Duvenaud \\
Department of Computer Science\\
University of Toronto\\
%Toronto, ON, Canada \\
\texttt{\{ccremer,duvenaud\}@cs.toronto.edu} \\
\texttt{\{quaid.morris\}@utoronto.ca}
% \texttt{ccremer@cs.toronto.edu},\texttt{quaid.morris@utoronto.ca},\texttt{duvenaud@cs.toronto.edu}\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood.
We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution. 

This distribution, $q_{IW}(z|x)$, is an importance weighted  implicitly defined by the  a pointwise reweighting of a parametric base distribution $q(z|x)$.

We formally derive this result, and examine properties of the implicit importance weighted posterior.
We also propose a family of extensions allowing iterative refinement of the approximate posterior based on gradients of the log-likelihood.
\end{abstract}
 

\begin{figure}[b]
  \centering
  True posterior \qquad $q_{IW}$ with $k=1$  \qquad $q_{IW}$ with $k=10$  \qquad $q_{IW}$ with $k=100$
  
      \includegraphics[width=0.9\textwidth, height=0.2\textwidth, clip, trim=0cm 1.5cm 0m 0cm]{figs/density_plot2.png}
  \caption{Approximate distributions to a complex true distribution. The approximations are defined via sampling-importance-resampling with k=1,10,100. As k grows, this approximation approaches the true distribution.}
  \label{viz1}
\end{figure}


\section{Background}
The importance-weighted autoencoder (IWAE; \cite{burda2015importance}) maximizes the following multi-sample evidence lower bound (ELBO): 
\begin{align} 
    log(p(x)) &
    % log\left(E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k \frac{p(x,z_i)}{q(z_i|x)}    \right]   \right)_{ML}
    % \\
    % & 
    \geq E_{z_{1}...z_{k} \sim q(z|x)} \left[log\left(  \frac{1}{k}\sum_{i=1}^k \frac{p(x,z_i)}{q(z_i|x)}  \right)  \right] \label{iwae_elbo}  \eqname{(IWAE ELBO)}
\end{align}
which is a tighter lower bound than the ELBO maximized by the variational autoencoder (VAE; \cite{vae}):
\begin{align}
    log(p(x)) & \geq E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{p(x,z_i)}{q(z_i|x)}  \right)  \right]. \label{vae_elbo} \eqname{(VAE ELBO)}
\end{align}
Here we've written the VAE bound as a multisample lower bound to compare it the IWAE bound. The following equations are the gradients of the VAE ELBO and the IWAE ELBO, respectively:
\begin{align} 
    \nabla_{\Theta} \mathcal{L}_{VAE}(x) &= E_{z_{1}...z_{k} \sim q(z|x)} \left[   \sum_{i=1}^k \frac{1}{k} \nabla_{\Theta} log\left(\frac{p(x,z_i)}{q(z_i|x)}  \right)  \right] \label{vae_grad} \\
% \end{align}
% \begin{align} 
    \nabla_{\Theta} \mathcal{L}_{IWAE}(x) &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \sum_{i=1}^k \tilde{w}_i \nabla_{\Theta} log\left(\frac{p(x,z_i)}{q(z_i|x)}  \right)  \right] \label{iwae_grad}
\end{align}
where $$\tilde{w}_i = \frac{\frac{p(x,z_i)}{q(z_i|x)}}{\sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)}}.$$
From Eqn. \ref{vae_grad}, we see that the gradient of the VAE ELBO evenly weights the samples whereas the IWAE gradient weights the samples based on their relative importance $\tilde{w}_i$.




\section{q\textsubscript{IW} distribution}

From the gradient of the IWAE bound (Eqn. \ref{iwae_grad}), we know that each sample is weighted by their importance $\tilde{w}_i$. This weighting implicitly defines a nonparametric approximate posterior. Given a batch of samples $z_{1}...z_{k}$ from $q(z|x)$, the following is the importance weighted $q$ distribution evaluated at $z_i$:
\begin{align} 
    q_{IW}(z_i|x,z_{\setminus i}) &= k \tilde{w}_i  q(z_i|x)\\
    &= \left( \frac{ \frac{p(x,z_i)}{q(z_i|x)}}{\frac{1}{k}   \sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)} } \right) q(z_i|x) \\
    &=  \frac{p(x,z_i)}{\frac{1}{k}   \sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)}} 
\end{align}
and the full distribution is given by:
\begin{align} 
    q_{IW}(z|x) &= E_{z_{1}...z_{k} \sim q(z|x)} \left[ q_{IW}(z_i|x) \right]. 
\end{align}\\
The implicit variational distribution of the IWAE ELBO is a stochasticaly weighted version of the variational distribution $q(z|x)$, where the optimal weighting is $p(z|x) / q(z|x)$. 
With $k=1$, the distribution is simply $q(z|x)$. When $k=\infty$, $q_{IW}(z|x)$ becomes the true posterior $p(z|x)$. See the Appendix for further details.


\subsection{$VAE+q_{IW} = IWAE$}

Here we show that the IWAE ELBO is equivalent to the VAE ELBO, but with a more flexible $q(z|x)$ distribution implicitly defined by importance reweighting. To show this, we plug in $q_{IW}(z_i|x,z_{\setminus i})$ for $q(z_i|x)$ into the VAE ELBO and see that it is equivalent to the IWAE ELBO:
\begin{align} 
    log(p(x)) \geq \mathcal{L}_{VAE} &= 
    E_{z \sim q(z|x)} \left[  log\left(\frac{p(x,z)}{q(z|x)} \right) \right] \\    
    &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{p(x,z_i)}{q(z|x)} \right) \right] \\
    &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{p(x,z_i)}{q_{IW}(z_i|x,z_{\setminus i})}  \right)  \right] \\
    &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{p(x,z_i)}{\frac{p(x,z_i)}{\frac{1}{k}   \sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)}}}  \right)  \right] \\
    &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{1}{k} \sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)}\right)  \right] \label{with_avg} \\
    &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  log\left(\frac{1}{k}\sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)}  \right)  \right] \label{without_avg} = \mathcal{L}_{IWAE}
\end{align}
Eqn. \ref{without_avg} follows Eqn. \ref{with_avg} since nothing is indexed by $i$ within the outer sum over indexes $i$. Eqn. \ref{without_avg} is the IWAE ELBO, thus we see that VAE with $q_{IW}$ is equivalent to the IWAE ELBO.












\section{Sampling q\textsubscript{IW}}
Due to the importance weighting of the lower bound, we can no longer sample from the orginal variational distribution $q(z|x)$; we must sample $q_{IW}$. The procedure to sample from $q_{IW}(z|x)$ is shown in Algo \ref{algo1}. It is equivalent to sampling-importance-resampling (SIR). What does $q_{IW}(z|x)$ look like? Fig \ref{viz1} demonstrates the sampling algorithm on a 2D distribution approximation problem. The true distribution is being approximated by a Gaussian $q$. We can see the shape of $q$ when we sample from it with $k=1$ (without resampling). As we increase the number of samples $k$ used for the sampling-resampling, the approximation approaches the true distribution.



\begin{figure*}[t]
\begin{minipage}[t]{0.49\columnwidth}
\begin{algorithm}[H]
\caption{Sampling from $q_{IW}$}\label{algo1}
\begin{algorithmic}[1]
    \State $\textit{k} \gets \textit{number of samples}$
    \State $q(z|x) = f_\phi(x)$
    \For {$i$ in $1 \dots k$}
        \State $z_i \sim q(z|x)$
        \State w_i = \frac{p(x,z_i)}{q(z_i|x)}$
    \EndFor    
    \State Each $\tilde w = w_i/\sum_{i=1}^{k} w_i$
    \State $j \sim Cat(\tilde{w})$
    \State Return $z_j$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\columnwidth}
\begin{algorithm}[H]
\caption{Sampling from recurrent $q_{IW}$}\label{algo1}
\begin{algorithmic}[1]
    \State $\textit{k} \gets \textit{number of samples}$
    \For {$i$ in $1 \dots k$}
        \State $q_i(z|x) = f_\phi(x, \nabla_z logp(z_{i-1}, x_{i-1}))$
        \State $z_i \sim q_i(z|x)$
        \State w_i = \frac{p(x,z_i)}{q(z_i|x)}$
    \EndFor    
    \State Each $\tilde w = w_i/\sum_{i=1}^{k} w_i$
    \State $j \sim Cat(\tilde{w})$
    \State Return $z_j$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\caption{}
\end{figure*}











\section{Discussion}

We showed that the IWAE lower bound can be interpreted as the standard VAE lower bound with a nonparametric, implicit $q$ distribution. In light of this, IWAE can be seen as increasing the complexity of the approximate distribution $q$, similar to other methods that increase the complexity of $q$, such as Normalizing Flows (\cite{normflow}), Variational Boosting (\cite{varboosting}), and Real NVP (\cite{realnvp}).


Sampling from the recognition distribution $q$ of the IWAE model can be split into two parts: sampling for training, and sampling for evaluation. During training, we sample the $q$ distribution and implicitly weight them with the IWAE ELBO. After training, we need to explicitly reweight samples from $q$.

In Fig. \ref{recon}, we demonstrate the need to sample from $q_{IW}$ rather than $q(z|x)$ for reconstructing MNIST digits. We trained the model to maximize the IWAE ELBO with K=50 and 2 latent dimensions, similar to Appendix C in \cite{burda2015importance}. When we sample from $q(z|x)$ and reconstruct the samples, we see a number of anomalies. However, if we perform the sampling-resampling step (Algo. \ref{algo1}), then the reconstructions are much more accurate. The intuition here is that we trained the model with $q_{IW}$ with $K=50$ then sampled from $q(z|x)$ ($q_{IW}$ with $K=1$), which are very different distributions, as seen in Fig. \ref{viz1}. 



\begin{figure}[ht]
  \centering
      \includegraphics[width=1.\textwidth]{figs/samps.png}
  \caption{Reconstructions of MNIST samples from $q(z|x)$ and $q_{IW}$. The model was trained by maximizing the IWAE ELBO with K=50 and 2 latent dimensions. The reconstructions from $q(z|x)$ are greatly improved with the sampling-resampling step of $qIW$.}
  \label{recon}
\end{figure}








\newpage

\section{Other}


\begin{itemize}
    \item We show that the IWAE lower bound can be interpreted as the standard VAE lower bound with a nonparametric, implicit $q$ distribution.
    \item We show that the sampling-resampling step, which samples from this implicit distribution, is necessary to produce samples from the variational posterior.  
    
    % This step resolves an anomaly in the original IWAE paper.
    \item We show that the IWAE framework can be extended to have richer families of base distributions, which can be jointly dependent or even based on recurrent neural networks.
\end{itemize}

% We notice that this point appears be overlooked in other works as well. \cite{maddison2016concrete}

We provided a different interpretation of IWAE: its the same elbo, but different q. 



 
% something about the rnn  \\
% This is important because .. \\
% fig one will be IWSI \\
% The only difference between the variational autoencoder (VAE; \cite{vae}) and the importance weighted autoencoder (IWAE; \cite{burda2015importance}) is their evidence lower bounds (ELBOs). Although the variational distribution $q_\phi(z|x)$ is typically a Normal distribution, the multi-sample IWAE ELBO implicitly defines a variational distribution that differs from a Normal distribution. (Something about people sample q not q iwae)\\ 
% Here we reinterpret the \\ 
% Here, we define the formula of the distribution, how to sample from it, visualize intermediate distributions, and samples from the distribution.\\
% The following are the multi-sample equations for the log evidence (marginal likelihood), IWAE ELBO, and VAE ELBO, respectively. 

%However, this interpretation is incomplete if we do not also consider that the implicit $q$ distribution is more complex.

% This is an analysis of the implicit variational distribution defined by the IWAE ELBO \citep{burda2015importance}. I define the formula of the distribution, how to sample from it, and visualize intermediate distributions, and samples from the distribution...

\newpage














% \subsubsection*{Acknowledgments}

% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{iclr2017_workshop}
\bibliographystyle{iclr2017_workshop}


\section{Appendix}


\subsection{\texorpdfstring{$\boldsymbol{k=1}$}{k=1}}

% The first sample needs to be z. 
When $k=1$, %the sample will be z, so
\begin{align} 
    q_{IW}(z|x) 
    % &= E_{z_{1} \sim q(z|x)} \left[\frac{\frac{p(x,z)}{q(z|x)}}{ \frac{p(x,z_1)}{q(z_1|x)}}  \right] q(z|x) \\
    % &= E_{z_{1} \sim q(z|x)} \left[1  \right] q(z|x) \\
    % &= E_{z_{1} \sim q(z|x)} \left[\frac{p(x,z)}{\frac{1}{1}\sum_{i=1}^1 \frac{p(x,z_i)}{q(z_i|x)}}  \right] \\
    % &= \frac{p(x,z)}{ \frac{p(x,z)}{q(z|x)}}  \\
    &= q(z|x)
\end{align}
Since $q_{IW}(z)=q(z|x)$, it is equivalent to VAE when $k=1$, as expected.











\subsection{\texorpdfstring{$\boldsymbol{k=\infty}$}{k=inf}} %

Recall that the marginal likehood can be approximated by:
\begin{align} 
    p(x) &= E_{q(z|x)}\left[\frac{p(x,z)}{q(z|x)} \right] \approx \frac{1}{k}\sum_i^k \frac{p(x,z_i)}{q(z_i|x)}
\end{align}
where $z_i$ is sampled from $q(z_i|x)$. Thus, when $k=\infty$:
\begin{align} 
    q_{IW}(z|x) &= E_{z_{1}...z_{\infty} \sim q(z|x)} \left[\frac{p(x,z)}{E_{q(z|x)}\left[\frac{p(x,z)}{q(z|x)} \right]}  \right] \\
    &= \frac{p(x,z)}{p(x)} \\
    &= p(z|x)
\end{align}
Thus $q_{IW}(z)$ is equal to the true posterior $p(z|x)$ when $k=\infty$, as expected.



\section{Visualizing q\textsubscript{IW} in 1D}

Now we can look at the intermediate variational distributions with different numbers of samples k. In Fig. \ref{viz} we see that as k increases, the approximate posterior approaches the true posterior. 

\begin{figure}[H]
  \centering
      \includegraphics[width=1.\textwidth]{figs/posteriors.png}
  \caption{Visualization of the importance weighted posterior. The blue distribution is the intractable distribution that we are trying to approximate. The green distribution is the variational distribution. The variational distributions of a, b, and c were optimized via SVI, whereas d, e, and f were optimized with a weighted SVI. The red histograms are weighted samples from the variational distribution.}
  \label{viz}
\end{figure}




% \begin{figure}[H]
% \centering
%     \begin{tabular}{ccccccccccccccccccccccccccccccc}
        
%         % 1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18&19&20\\
%         &Real&&&&&&Sample $q(z|x)$&&&&&&&&&Sample $q_{IW}(z|x)$&&&&&&&&&&&\\        
%         % \cline{2-2} \cline{4-12} \cline{14-21}
        
%         \multicolumn{30}{c}{\includegraphics[scale=.7]{figs/recon_iwae.png}}\\
        
%         % a&c&c&d&c&d

%     \end{tabular}
% \caption{Visualization of the reconstructions of samples from }
% \label{recon}
% \end{figure}





\end{document}











% \begin{figure}[H]
% % \begin{table}[ht]

% \centering
% % \renewcommand{\arraystretch}{3}
%     \begin{tabular}{cccc}
        
%         % & \multicolumn{1}{c}{VAE} & & \multicolumn{4}{c}{IWAE} \\
        
%         & Sample $q(z|x)$ & & Sample $q_{IWAE}$ \\
%         \cline{2-2}  \cline{4-4} \\
%         % \rule{0pt}{3ex}
        
%         % 1 & 2 & 3 & 4 \\
        
%         % \rotatebox{90}{VAE\ }\vline  & \multirow{2}{*}{\includegraphics[width=45mm]{figs/recon_samplingvae.png}} & & \multirow{2}{*}{\includegraphics[width=45mm]{figs/recon_samplingiwae.png}} \\[4.5ex]
        
%         % \rotatebox{90}{IWAE\ }\vline & & & \\[4.5ex]        
        
%         % \cline{1-1}
    
%         VAE\ \ \ \ \vline  & \multirow{2}{*}{\includegraphics[width=45mm]{figs/recon_samplingvae.png}} & & \multirow{2}{*}{\includegraphics[width=45mm]{figs/recon_samplingiwae.png}} \\[9.5ex]
        
%         \\
        
%         IWAE\ \vline & & & \\[9ex]
        
%         \\
%     \end{tabular}
% \caption{Visualization of the reconstructions of samples from $q(z|x)$ (left) and $qIWAE$ (right). K indicates the number of samples and VAE/IWAE indicates the ELBO used during training. 2latent dimensions, k=50}
% % \label{tab:gt}
% % \end{table}
% \label{recon}
% \end{figure}



% q in elbo

% \begin{align} 
%     log(p(x)) & \geq E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{p(x,z_i)}{q_{IWAE}(z_i|x)}  \right)  \right] \\
%     &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{p(x,z_i)}{E_{z_{1}...z_{k} \sim q(z|x)} \left[\frac{p(x,z_i)}{\frac{1}{k}\sum_i^k \frac{p(x,z_j)}{q(z_j|x)}}  \right]}  \right)  \right] \label{with_e} \\
%     &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{p(x,z_i)}{\left(\frac{p(x,z_i)}{\frac{1}{k}\sum_j^k \frac{p(x,z_j)}{q(z_j|x)}}  \right)}  \right)  \right] \label{without_e} \\
%     &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  \frac{1}{k}\sum_{i=1}^k log\left(\frac{1}{k}\sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)}  \right)  \right] \label{with_avg} \\
%     &= E_{z_{1}...z_{k} \sim q(z|x)} \left[  log\left(\frac{1}{k}\sum_{j=1}^k \frac{p(x,z_j)}{q(z_j|x)}  \right)  \right] \label{without_avg}
% \end{align}


% The  standard  interpretation  of  importance-weighted  autoencoders  is  that  they maximize a tighter lower bound on the marginal likelihood.  We give an alternate interpretation: that it optimizes the standard variational lower bound, but using a more complex distribution. This distribution is a stochastic pointwise reweighting of the base variational distribution.  We formally derive this result, and examine properties of the implicit importance weighted posterior.  We also stress the needto use a resampling step to sample from the approximate posterior trained usingthe IWAE objective